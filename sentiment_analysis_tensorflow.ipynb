{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPs+6lrMJpL1vo6ma+sht4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shdb1/deep_learning/blob/main/sentiment_analysis_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_wvbBr2I22g"
      },
      "source": [
        "### Data Prep\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9UXwu_QG_h5"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzwto85YIkfu"
      },
      "source": [
        "lemm = WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9KnfOSCIrKk",
        "outputId": "8c099d58-332c-42de-b7ab-49fcebdf2c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "def rand_list(lines, max_value):\n",
        "    randlist = []\n",
        "    for _ in range(lines):\n",
        "        num = random.randint(0, max_value-1)\n",
        "        while num in randlist:\n",
        "            num = random.randint(0, max_value - 1)\n",
        "        randlist.append(num)\n",
        "\n",
        "    return randlist\n",
        "\n",
        "\n",
        "def shuffler(input_ds, output_ds):\n",
        "    df_source = pd.read_csv(input_ds, '<SP>', error_bad_lines=False)\n",
        "    df_shuffled = df_source.iloc[np.random.permutation(len(df_source))]\n",
        "    # print(df_shuffled.head())\n",
        "    df_shuffled.to_csv(output_ds, 'µ', index=False)\n",
        "\n",
        "\n",
        "def smaller_dataset_gen(ds, newds, dsrows, num_lines=1000):\n",
        "    count = 0\n",
        "    with open(ds, 'r', 5000, 'latin-1') as raw_ds:\n",
        "        with open(newds, 'w', 5000) as target_ds:\n",
        "            selected_lines = rand_list(num_lines, dsrows)\n",
        "            for line in raw_ds:\n",
        "                if len(selected_lines) == 0:\n",
        "                    break\n",
        "\n",
        "                if count in selected_lines:\n",
        "                    target_ds.write(line)\n",
        "                    selected_lines.remove(count)\n",
        "                count += 1\n",
        "\n",
        "    print(\"New dataset created with {} lines\".format(num_lines))\n",
        "\n",
        "\n",
        "def clean_dataset(ds, ods):\n",
        "    with open(ds, 'r', 30000, 'latin-1') as raw_ds:\n",
        "        with open('tempds.csv', 'w', 20000) as cleaned_ds:\n",
        "            for line in raw_ds:\n",
        "                result = re.search('^\"(\\d)\",.*,\"(.*)\"$', line)\n",
        "                new_line = result.group(1) + '<SP>' + result.group(2) + '\\n'\n",
        "                cleaned_ds.write(new_line)\n",
        "\n",
        "        shuffler('tempds.csv', ods)\n",
        "        os.remove('tempds.csv')\n",
        "    print(\"Dataset cleanup done\")\n",
        "\n",
        "\n",
        "# Responsible to create a list with all the words that matter already lemmatized\n",
        "def create_word_dict(source_ds):\n",
        "    word_dict = []\n",
        "    with open(source_ds, 'r', 30000, 'latin-1') as ds:\n",
        "        for line in ds:\n",
        "            text = line.split('µ')[1]\n",
        "            words = word_tokenize(text.lower())\n",
        "            lemm_words = [lemm.lemmatize(w) for w in words]\n",
        "            word_dict += list(lemm_words)\n",
        "\n",
        "        word_count = Counter(word_dict)\n",
        "\n",
        "    cleaned_word_dict = [word for word in word_count if 1000 > word_count[word] > 60]\n",
        "    dict_size = len(cleaned_word_dict)\n",
        "\n",
        "    print(\"Word dictionary size: {}\".format(dict_size))\n",
        "    with open('word_dict.pickle', 'wb') as wd:\n",
        "        pickle.dump(cleaned_word_dict, wd)\n",
        "\n",
        "    print(\"Word dictionary generated and saved\")\n",
        "    return dict_size\n",
        "\n",
        "\n",
        "# Prepares the sentences changing them into the hot vector\n",
        "def sentence_to_vector(word_dict_file, cleaned_ds, output_file):\n",
        "\n",
        "    with open(cleaned_ds, 'r', 30000, 'latin-1') as ds:\n",
        "        with open(word_dict_file, 'rb') as wd:\n",
        "            word_dict = pickle.load(wd)\n",
        "            num_lines = 0\n",
        "            # print(len(word_dict))\n",
        "            # print(word_dict)\n",
        "            with open(output_file, 'wb') as hv:\n",
        "\n",
        "                for line in ds:\n",
        "                    # print(line)\n",
        "                    hot_vector = np.zeros(len(word_dict))\n",
        "                    if line.count('µ') == 1:\n",
        "                        sentiment, text = line.split('µ')\n",
        "                        words = word_tokenize(text.lower())\n",
        "                        lemm_words = [lemm.lemmatize(w) for w in words]\n",
        "                        for word in lemm_words:\n",
        "                            if word in word_dict:\n",
        "                                hot_vector[word_dict.index(word)] += 1\n",
        "                        hot_vector = list(hot_vector)\n",
        "\n",
        "                        clean_sentiment = re.search('.*(\\d).*', sentiment)\n",
        "\n",
        "                        if int(clean_sentiment.group(1)) == 0:\n",
        "                            sentiment = [1, 0]\n",
        "                        else:\n",
        "                            sentiment = [0, 1]\n",
        "\n",
        "                        # print(hot_vector, sentiment)\n",
        "                        num_lines += 1\n",
        "\n",
        "                        pickle.dump([hot_vector, sentiment], hv)\n",
        "\n",
        "                print('Hot vectors file generated with {} lines'.format(num_lines))\n",
        "    return num_lines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-79f5f17ad24a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_details.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mdict_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_word_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'small_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'small_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_hot_vectors.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_hot_vectors.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-79f5f17ad24a>\u001b[0m in \u001b[0;36mcreate_word_dict\u001b[0;34m(source_ds)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_word_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'µ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'small_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swtB3255IuC5"
      },
      "source": [
        "# smaller_dataset_gen('trainingandtestdata/training.1600000.processed.noemoticon.csv', 'smaller_ds.csv', 1600000, 10000)\n",
        "# clean_dataset('trainingandtestdata/training.1600000.processed.noemoticon.csv', 'result.csv')\n",
        "# clean_dataset('smaller_ds.csv', 'small_train.csv')\n",
        "# clean_dataset('trainingandtestdata/testdata.manual.2009.06.14.csv', 'test.csv')\n",
        "\n",
        "with open('data_details.pkl', 'wb') as details:\n",
        "    dict_size = create_word_dict('small_train.csv')\n",
        "    train_size = sentence_to_vector('word_dict.pickle', 'small_train.csv', 'train_hot_vectors.pickle')\n",
        "    test_size = sentence_to_vector('word_dict.pickle', 'test.csv', 'test_hot_vectors.pickle')\n",
        "    details_sizes = {'dict': dict_size, 'train': train_size, 'test': test_size}\n",
        "    pickle.dump(details_sizes, details)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaNx7JjGJMvl"
      },
      "source": [
        "###Build NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIH_5wsiI1sN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6-ucynLJSr2"
      },
      "source": [
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "batch_size = 1000\n",
        "num_epochs = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFSzF6hJVaU"
      },
      "source": [
        "def load_details():\n",
        "    with open('data_details.pkl', 'rb') as details:\n",
        "        det = pickle.load(details)\n",
        "        return det"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drAhiY4JJYJD"
      },
      "source": [
        "line_sizes = load_details()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSqmlDT9JaCN"
      },
      "source": [
        "# Creates the neural network model\n",
        "def ff_neural_net(input_data):\n",
        "    neurons_hl1 = 1500\n",
        "    neurons_hl2 = 1500\n",
        "    neurons_hl3 = 1500\n",
        "\n",
        "    output_neurons = 2\n",
        "\n",
        "    l1_weight = tf.Variable(tf.random_normal([line_sizes['dict'], neurons_hl1]), name='w1')\n",
        "    l1_bias = tf.Variable(tf.random_normal([neurons_hl1]), name='b1')\n",
        "\n",
        "    l2_weight = tf.Variable(tf.random_normal([neurons_hl1, neurons_hl2]), name='w2')\n",
        "    l2_bias = tf.Variable(tf.random_normal([neurons_hl2]), name='b2')\n",
        "\n",
        "    l3_weight = tf.Variable(tf.random_normal([neurons_hl2, neurons_hl3]), name='w3')\n",
        "    l3_bias = tf.Variable(tf.random_normal([neurons_hl3]), name='b3')\n",
        "\n",
        "    output_weight = tf.Variable(tf.random_normal([neurons_hl3, output_neurons]), name='wo')\n",
        "    output_bias = tf.Variable(tf.random_normal([output_neurons]), name='bo')\n",
        "\n",
        "    l1 = tf.add(tf.matmul(input_data, l1_weight), l1_bias)\n",
        "    l1 = tf.nn.relu(l1)\n",
        "\n",
        "    l2 = tf.add(tf.matmul(l1, l2_weight), l2_bias)\n",
        "    l2 = tf.nn.relu(l2)\n",
        "\n",
        "    l3 = tf.add(tf.matmul(l2, l3_weight), l3_bias)\n",
        "    l3 = tf.nn.relu(l3)\n",
        "\n",
        "    output = tf.matmul(l3, output_weight) + output_bias\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inINWqqqJciz"
      },
      "source": [
        "def training(in_placeholder):\n",
        "    nn_output = ff_neural_net(in_placeholder)\n",
        "    saver = tf.train.Saver()\n",
        "    # We are using cross entropy to calculate the cost\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=nn_output, labels=y))\n",
        "\n",
        "    # and Gradient Descent to reduce the cost\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "    # A TensorFLow session is created that will actually run the previously defined graph\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        # saver = tf.train.Saver()\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_loss = 0\n",
        "            buffer_train = []\n",
        "            buffer_label = []\n",
        "            with open('train_hot_vectors.pickle', 'rb') as train_hot_vec:\n",
        "                for i in range(line_sizes['train']):\n",
        "                    hot_vector_line = pickle.load(train_hot_vec)\n",
        "                    buffer_train.append(hot_vector_line[0])\n",
        "                    buffer_label.append(hot_vector_line[1])\n",
        "\n",
        "                    # print('Bla:' + str(buffer_label))\n",
        "\n",
        "                    if len(buffer_train) >= batch_size:\n",
        "                        _, cost_iter = sess.run([optimizer, cost],\n",
        "                                                feed_dict={in_placeholder: buffer_train, y: buffer_label})\n",
        "                        epoch_loss += cost_iter\n",
        "                        buffer_train = []\n",
        "                        buffer_label = []\n",
        "\n",
        "            print('Epoch {} completed. Total loss: {}'.format(epoch+1, epoch_loss))\n",
        "\n",
        "        correct = tf.equal(tf.argmax(nn_output, 1), tf.argmax(y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "\n",
        "        with open('test_hot_vectors.pickle', 'rb') as train_hot_vec:\n",
        "            buffer_test = []\n",
        "            buffer_test_label = []\n",
        "            for i in range(line_sizes['test']):\n",
        "                test_hot_vector_line = pickle.load(train_hot_vec)\n",
        "                buffer_test.append(test_hot_vector_line[0])\n",
        "                buffer_test_label.append(test_hot_vector_line[1])\n",
        "\n",
        "        # the accuracy is the percentage of hits\n",
        "        print('Accuracy using test dataset: {}'\n",
        "              .format(accuracy.eval({in_placeholder: buffer_test, y: buffer_test_label})))\n",
        "        # saver = tf.train.Saver()\n",
        "        saver.save(sess, \"model.ckpt\")\n",
        "\n",
        "\n",
        "# training(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbrOKBFFJl6G"
      },
      "source": [
        "###Use NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEv0_zaeJhAI"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sentiment_neural_net import ff_neural_net\n",
        "from sentiment_neural_net import training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd8bo955Jpzn"
      },
      "source": [
        "lemm = WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Py_HDuaJsFj"
      },
      "source": [
        "# function responsible to receive a sentence, prepare it (tokenizing, lemmatizing and tranforming into the hot vector\n",
        "def get_sentiment(input_data):\n",
        "    tf.reset_default_graph()\n",
        "    pl = tf.placeholder('float')\n",
        "    nn_output = ff_neural_net(pl)\n",
        "    saver = tf.train.Saver()\n",
        "    with open('word_dict.pickle', 'rb') as f:\n",
        "        word_dict = pickle.load(f)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # sess.run(tf.global_variables_initializer())\n",
        "        # saver = tf.train.Saver()\n",
        "        saver.restore(sess, \"model.ckpt\")\n",
        "        words = word_tokenize(input_data.lower())\n",
        "        lemm_words = [lemm.lemmatize(w) for w in words]\n",
        "        hot_vector = np.zeros(len(word_dict))\n",
        "\n",
        "        for word in lemm_words:\n",
        "            if word.lower() in word_dict:\n",
        "                index_value = word_dict.index(word.lower())\n",
        "                hot_vector[index_value] += 1\n",
        "\n",
        "        hot_vector = np.array(list(hot_vector))\n",
        "\n",
        "        result = (sess.run(tf.argmax(nn_output.eval(feed_dict={pl: [hot_vector]}), 1)))\n",
        "        # print(result)\n",
        "        if result[0] == 0:\n",
        "            print('Negative:', input_data)\n",
        "        elif result[0] == 1:\n",
        "            print('Positive:', input_data)\n",
        "\n",
        "\n",
        "# Uncomment the row below to train the model\n",
        "# training(x)\n",
        "\n",
        "# call the 'use_neural_network' providing a sentence to check the neural network return\n",
        "get_sentiment('Lebron is a beast... nobody in the NBA comes even close')\n",
        "get_sentiment(\"This was the best store i've ever seen.\")\n",
        "get_sentiment(\"Why do you hate the world\")\n",
        "get_sentiment(\"we always need to do good things to help each other\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}